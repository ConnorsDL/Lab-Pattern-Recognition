{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92643c93",
   "metadata": {
    "editable": false,
    "user_expressions": []
   },
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart Kernel) and then **run all cells** (in the menubar, select Run$\\rightarrow$Run All Cells). Alternatively, you can use the **validate** button in the assignment list panel.\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\". When you insert your Code you can remove the line `raise NotImplementedError()`. Also put your name, matriculationnumber, and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2116dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "MATRICULATIONNUMBER = \"\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadc3f36",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4718055-99e5-4bd3-b5f7-739208205745",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c278240211b4050d6b0e4afe56d50152",
     "grade": false,
     "grade_id": "22-intro-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "user_expressions": []
   },
   "source": [
    "<img src=\"images/logo_ifn.svg\" alt=\"Drawing\" style=\"width: 256px;\" align=\"right\"/>\n",
    "\n",
    "# Exercise 2.2: Simple 2D Problems\n",
    "\n",
    "In the last notebook you got to know the basics of machine learning. You learned how to generate, prepare, and preprocess data for machine learning models as well as training and evaluation of such models. Also, the last notebook shows that not for every problem a sophisticated deep neural network is needed, which you should keep in mind when encountering data-related problems in the future. In this notebook, we will further explore simple 2D problems with support vector machines and first single-layer neural networks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8f571d5-867d-4c8b-978d-08ee646c4831",
   "metadata": {
    "deletable": false,
    "editable": false,
    "init_cell": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6ef9e878d29da442d087022ed9f88089",
     "grade": false,
     "grade_id": "22-imports",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scene__Default Scene": true,
    "scene__Legacy Init": true,
    "tags": [
     "ActiveScene"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.datasets import make_swiss_roll\n",
    "from sklearn import svm\n",
    "from sklearn import neural_network\n",
    "import matplotlib.pyplot as plt\n",
    "import params\n",
    "from ipylab import JupyterFrontEnd\n",
    "from threadpoolctl import threadpool_limits\n",
    "\n",
    "PARAM_1, PARAM_2, PARAM_3 = params.gen_params(os.getcwd())\n",
    "PARAM_4 = int(params.gen_params(os.getcwd(), mode='float', num=1)[0] *100000)\n",
    "app = JupyterFrontEnd()\n",
    "app.commands.execute('notebook:render-all-markdown')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515109a8-d7ff-4665-ab39-c58dcc4054a9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5d6da37d431567771799b7c1c0637375",
     "grade": false,
     "grade_id": "22-a-task",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "user_expressions": [
     {
      "expression": "PARAM_4",
      "result": {
       "data": {
        "text/plain": "13832"
       },
       "metadata": {},
       "status": "ok"
      }
     },
     {
      "expression": "PARAM_4*2",
      "result": {
       "data": {
        "text/plain": "27664"
       },
       "metadata": {},
       "status": "ok"
      }
     }
    ]
   },
   "source": [
    "<img src=\"images/2d-distribution.png\" alt=\"Drawing\" style=\"width: 256px;\" align=\"right\"/>\n",
    "\n",
    "### Task 2.2-A: Data Generation (5P) \n",
    "\n",
    "The aim of this task is to generate a data distribution as shown on the right. For this purpose, below you can find two functions. Look through the functions and familiarize yourself with them. Execute the cell with the `generate_distribution` function. It will yield two Gaussian clusters as shown in the resulting plot. Modify the data generation according to the following steps to generate the data distribution shown on the right hand side.\n",
    "- Generate four Gaussian clusters of class 0 with centers at (2, 2), (-2, 2), (-2, -2), and (2, -2), identity covariance matrix, and 100 samples per cluster.\n",
    "- Generate four Gaussian clusters of class 1 with centers at (0, 2), (-2, 0), (2, 0), and (0, -2), diagonal covariance matrix which only contains the values 0.1 and 2, and 100 samples per cluster.\n",
    "- Multiply all x-values with a value of 1000 while you multiply all y-values with a value of 0.1 for both training and (later generated) validation data\n",
    "- Generate a validation set with the same parameters. Use a seed of {{PARAM_4}} for generation of the training set and a seed of {{PARAM_4*2}} for generation of the validation set.\n",
    "- Keep only 10\\% of values from class 1 in the training dataset. This should yield a similar distribution as shown on the right hand side. Retain the same amount of samples for both classes in the validation set. The validation set should contain 400 samples per class at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8490bd4-c837-45ee-a15a-15f06278f78c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b9ea176bc1244dc1a16cf8b64f7a417",
     "grade": false,
     "grade_id": "22-a-functions",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def genGauss(means, covs, pre_label, amount_samples, seed):\n",
    "    \"\"\"This function generates Gaussian 2D data clusters with axis-specific means and covariances as parameters\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    X = []\n",
    "    Y = []\n",
    "    if ((len(means.shape) <= 1) or (len(covs.shape) <= 1)):\n",
    "        exit(\"Dimension of Data (means or cov-matrices) not equal.\\nPlease ensure that dimensions of mean and cov-matrices fit!\")\n",
    "    if pre_label.size == 0 and amount_samples.size == 1:\n",
    "        if not (means.shape[0] == covs.shape[0]):\n",
    "            exit(\n",
    "                \"Number of means do not fit to the number of cov-matrices!\\nPlease ensure that the amounts are equal.\")\n",
    "    elif pre_label.size == 0:\n",
    "        if not (means.shape[0] == covs.shape[0] == amount_samples.shape[0]):\n",
    "            exit(\n",
    "                \"Number of means do not fit to the number of cov-matrices or labels!\\nPlease ensure that the amounts are equal.\")\n",
    "    elif amount_samples.size == 1:\n",
    "        if not (means.shape[0] == covs.shape[0] == pre_label.shape[0]):\n",
    "            exit(\n",
    "                \"Number of means do not fit to the number of cov-matrices or amount of samples!\\nPlease ensure that the amounts are equal.\")\n",
    "    else:\n",
    "        if not (means.shape[0] == covs.shape[0] == pre_label.shape[0] == amount_samples.shape[0]):\n",
    "            exit(\"Number of means do not fit to the number of cov-matrices or labels or amount of samples!\\nPlease ensure that the amounts are equal.\")\n",
    "    for i in range(0, means.shape[0]):\n",
    "        if amount_samples.size == 1:\n",
    "            gauss_vals = np.random.multivariate_normal(means[i], covs[i], amount_samples[0])\n",
    "        elif amount_samples.size < 1:\n",
    "            exit(\n",
    "                \"Amount of samples < 1!\\nPlease ensure that the amount is at least 1.\")\n",
    "        else:\n",
    "            gauss_vals = np.random.multivariate_normal(means[i], covs[i], amount_samples[i])\n",
    "        labels = np.empty([gauss_vals.shape[0]])\n",
    "        if pre_label.size == 0:\n",
    "            labels.fill(i)\n",
    "        else:\n",
    "            labels.fill(pre_label[i])\n",
    "        for k in range(0, gauss_vals.shape[0]):\n",
    "            X.append(gauss_vals[k])\n",
    "            Y.append(labels[k])\n",
    "    combined = list(zip(X, Y))\n",
    "    random.shuffle(combined)\n",
    "    X[:], Y[:] = zip(*combined)\n",
    "    return np.asarray(X), np.asarray(Y)    \n",
    "    \n",
    "def modify_data(X, Y, multipliers=(1, 1), imbalance=1):\n",
    "    \"\"\"This function multiplies data given by X and Y with the respecitve multiplyer elements. \n",
    "    It also deletes elements according to the imbalance parameter.\"\"\"\n",
    "    np.random.seed(0)\n",
    "    random.seed(0)\n",
    "    X[:,0] *= multipliers[0]\n",
    "    X[:,1] *= multipliers[1]\n",
    "    X_N = []\n",
    "    Y_N = []\n",
    "    classes = np.unique(Y)\n",
    "    num_samples = int(len(X)/len(classes))\n",
    "    for cl in classes:\n",
    "        mask = (Y==cl)\n",
    "        if cl == 1:\n",
    "            X_N.append(X[mask,:][:int(num_samples*imbalance)])\n",
    "            Y_N.append(Y[mask][:int(num_samples*imbalance)])\n",
    "        else:\n",
    "            X_N.append(X[mask,:][:num_samples])\n",
    "            Y_N.append(Y[mask][:num_samples])\n",
    "    X_N = np.concatenate(X_N)\n",
    "    Y_N = np.concatenate(Y_N)\n",
    "    p = np.random.permutation(len(X_N))\n",
    "    X_N, Y_N = X_N[p], Y_N[p]\n",
    "    return X_N, Y_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe38865-3f9e-43c6-bf26-3326f4780b46",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1b868a8257a0fa1dc210213dbed38f18",
     "grade": false,
     "grade_id": "22-a-answer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def generate_distribution(multipliers, imbalance):\n",
    "    # the below code is just a simple example on how to use the above functions.\n",
    "    # You can edit it as necessary for solving the task\n",
    "    amount_samples_train = np.array([100, 100, 100, 100, 100, 100])  \n",
    "    means_train = np.array([[-2, -2], [2, 2], [0, 2], [-2, 0], [2, 0], [0, -2]])\n",
    "    covs_train = np.array([[[1, 0], [0, 1]]] * 4 + [[[0.1, 0], [0, 2]]] * 2)\n",
    "    labels_train = np.array([0, 0, 0, 0, 1, 1])\n",
    "    X_tr, Y_tr = genGauss(means_train, covs_train, labels_train, amount_samples_train, seed=13832)\n",
    "    X_tr, Y_tr = modify_data(X_tr, Y_tr, multipliers, imbalance)\n",
    "    \n",
    "    amount_samples_val = np.array([400, 400]) \n",
    "    X_val, Y_val = genGauss(means_train, covs_train, labels_train, amount_samples_val, seed=27664)\n",
    "    X_val, Y_val = modify_data(X_val, Y_val, multipliers, imbalance)\n",
    "    # YOUR CODE HERE\n",
    "   \n",
    "    return X_tr, Y_tr, X_val, Y_val\n",
    "\n",
    "X_tr, Y_tr, X_val, Y_val = generate_distribution((1000, 0.1),0.1)\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "plt.scatter(X_tr[:,0], X_tr[:,1], c=Y_tr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f91324-0995-4e0c-8887-d1d2e73d641e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8a2c16a7fa6f8a361c93375dbe46ec52",
     "grade": true,
     "grade_id": "22-a-test",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(X_tr) == np.ndarray\n",
    "assert type(Y_tr) == np.ndarray\n",
    "assert type(X_val) == np.ndarray\n",
    "assert type(Y_val) == np.ndarray\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf23966c-1fcb-4f59-a26d-3c2289f6b4c0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dd8bdba3b0ff3ea38609cda0de0d0e15",
     "grade": false,
     "grade_id": "22-b-task",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "user_expressions": []
   },
   "source": [
    "### Task 2.2-B: Support Vector Machines (5P) \n",
    "\n",
    "The generated data is clearly not linearly separable. In a first attempt to solving this problem, implement a support vector machine to solve this problem. For this purpose complete the function below taking the training data and labels as well as several hyperparameters as input and returning the trained SVM. Make yourself familiar with the SVM implementation of sklearn and implement an SVM that achieves 100% accuracy on the training set. Discuss with your fellow students why this is not the optimal solution and what the reasons might be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645ec1b2-ad79-463f-a0a6-9625189fe963",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c4e7d71808b213c36262af50a056c057",
     "grade": false,
     "grade_id": "22-b-answer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_svm(X_tr, Y_tr, kernel_function, penalty, kernel_coeff, max_iterations):\n",
    "    # YOUR CODE HERE\n",
    "    clf = SVC(kernel=kernel_function,C=penalty,gamma = kernel_coeff,max_iter = max_iterations)\n",
    "    clf.fit(X_tr,Y_tr)\n",
    "    return clf\n",
    "\n",
    "with threadpool_limits(limits=1, user_api='blas'):\n",
    "    clf = train_svm(X_tr, Y_tr, kernel_function='linear', penalty=1.0, kernel_coeff=10.0, max_iterations=10000)\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    print('Result svm-Training - Accuracy on training data = ' + '{0:g}%'.format(clf.score(X_tr, Y_tr) * 100) + \"\\n\")\n",
    "    print('Result svm-Validation - Accuracy on validation data = ' + '{0:g}%'.format(clf.score(X_val, Y_val) * 100) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27852cd5-34ff-4333-b10e-391758d83fb7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2f985cabc6a3c19a3b424dc56ea98424",
     "grade": true,
     "grade_id": "22-b-test",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "train_accuracy = clf.score(X_tr, Y_tr)\n",
    "print(train_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70b7f9f-485d-4ecf-bf35-6a8f8c28addb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6f5ad0f1d30ba7eb14e19f46012fb4a9",
     "grade": false,
     "grade_id": "22-c-task",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "user_expressions": []
   },
   "source": [
    "### Task 2.2-C: Class Balancing and Data Normalization (5P) \n",
    "\n",
    "You might have received a ConvergenceWarning in the previous task, hinting at a rather poor convergence of the model. Unnormalized data as well as the class imbalance can be possible reasons for the poor convergence and the rather bad results on the validation set. Regenerate your data using the `generate_distribution` function (it should be reproducible due to the set random seeds) and see how the class imbalance as well as the unnormalized and inhomogeneous value ranges affect the SVM training and generalization result. Find a way to improve the SVM result on the validation set to more than 75\\%. Also, discuss with your fellow students how the hyperparameters affect the training and validation scores and which commonly known phenomenon you can observe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fc7425-c999-4b94-9ab0-89b128ee6713",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "46f8fd191b8a81a6a53ad916a5a7c6fe",
     "grade": false,
     "grade_id": "22-c-answer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_tr, Y_tr, X_val, Y_val = generate_distribution((1000, 0.1), 0.1)\n",
    "clf = train_svm(X_tr, Y_tr, kernel_function='linear', penalty=1.0, kernel_coeff=10.0, max_iterations=10000)\n",
    "\n",
    "with threadpool_limits(limits=1, user_api='blas'):\n",
    "    # YOUR CODE HERE\n",
    "    scaler = StandardScaler()\n",
    "    X_tr_hormalized = scaler.fit_tranform(X_tr)\n",
    "    X_val_normalized = scaler.transform(X_val)\n",
    "    \n",
    "    class_weights = compute_class_weight('balanced',classes = np.unique(Y_tr),y=Y_tr)\n",
    "    \n",
    "    clf.fit(X_tr_normalized, Y_tr, sample_weight=np.array([class_weights[int(label)] for label in Y_tr]))\n",
    "    print('Result svm-Training - Accuracy on training data = ' + '{0:g}%'.format(clf.score(X_tr, Y_tr) * 100) + \"\\n\")\n",
    "    print('Result svm-Validation - Accuracy on validation data = ' + '{0:g}%'.format(clf.score(X_val, Y_val) * 100) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f2a64a-1cef-483e-8261-5c296cd46241",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7aa2e50f7ad1fb04d84367870abe73b7",
     "grade": true,
     "grade_id": "22-c-test",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "validation_accuracy = clf.score(X_val, Y_val)\n",
    "print(validation_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f6b9d5-f681-4d7b-a37a-6427d10f6505",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aef24583b37bd2dda58efbb2575f39a4",
     "grade": false,
     "grade_id": "22-d-task",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "user_expressions": [
     {
      "expression": "PARAM_4",
      "result": {
       "data": {
        "text/plain": "13832"
       },
       "metadata": {},
       "status": "ok"
      }
     }
    ]
   },
   "source": [
    "### Task 2.2-D: Single-Layer Neural Networks (5P) \n",
    "\n",
    "Let's start implementing our first neural network. We will use the neural network to solve the same task that we previously approached with the SVM. To this end, please implement the following:\n",
    "- Complete the `train_nnet` function by first instantiating the `MLPClassifier` class with the parameters passed to the `train_nnet` function and afterwards fitting it to the training data. Return the trained model as `clf`.\n",
    "- As you will see the accuracy on both the training and the validation set is rather bad. Try to find model parameters that achieve a training score of more than 85\\% (don't change the data at this point). Use a random_state value of {{PARAM_4}}. Discuss why the validation is not easily improved. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bc2509-328b-4395-819f-73b31503b521",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b14f7097b2d14cf3aa2b0d378d1f9b9a",
     "grade": false,
     "grade_id": "22-d-answer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_nnet(X_tr, Y_tr, \n",
    "               hidden_layer_sizes, \n",
    "               activation, \n",
    "               solver,\n",
    "               alpha,\n",
    "               batch_size,\n",
    "               learning_rate,\n",
    "               max_iter,\n",
    "               learning_rate_init=0.001, \n",
    "               shuffle=True, \n",
    "               nesterovs_momentum=True, \n",
    "               momentum=0.9,\n",
    "               random_state=0):\n",
    "    np.random.seed(random_state)\n",
    "    random.seed(random_state)\n",
    "    # YOUR CODE HERE\n",
    "    clf = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes,\n",
    "                        activation=activation,\n",
    "                        solver=solver,\n",
    "                        alpha=alpha,\n",
    "                        batch_size=batch_size,\n",
    "                        learning_rate=learning_rate,\n",
    "                        max_iter=max_iter,\n",
    "                        learning_rate_init=learning_rate_init,\n",
    "                        shuffle=shuffle,\n",
    "                        nesterovs_momentum=nesterovs_momentum,\n",
    "                        momentum=momentum,\n",
    "                        random_state=random_state)\n",
    "    clf.fit(X_tr,Y_tr)\n",
    "    return clf\n",
    "\n",
    "with threadpool_limits(limits=1, user_api='blas'):\n",
    "    X_tr, Y_tr, X_val, Y_val = generate_distribution((1000, 0.1), 0.1)\n",
    "    clf = train_nnet(X_tr, Y_tr, \n",
    "                     hidden_layer_sizes = (100),\n",
    "                     activation = \"identity\", \n",
    "                     solver = \"sgd\",\n",
    "                     alpha = 0.0001,\n",
    "                     batch_size = 200,\n",
    "                     learning_rate = \"constant\",\n",
    "                     max_iter = 500\n",
    "                     learning_rate_init=0.001,\n",
    "                     shuffle=True,\n",
    "                     nesterovs_momentum=True,\n",
    "                     momentum=0.9,\n",
    "                     random_state=13832)\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "\n",
    "    print('Result nnet-Training - Accuracy on training data = ' + '{0:g}%'.format(clf.score(X_tr, Y_tr) * 100) + \"\\n\")\n",
    "    print('Result nnet-Validation - Accuracy on validation data = ' + '{0:g}%'.format(clf.score(X_val, Y_val) * 100) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3f795c-ea41-4512-b66e-f39f2995eda0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "51bf1cfd8fa3ba082bf1f1ffdd2b7316",
     "grade": true,
     "grade_id": "22-d-test",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_tr, Y_tr, X_val, Y_val = generate_distribution((1000, 0.1), 0.1)\n",
    "train_accuracy = clf.score(X_tr, Y_tr)\n",
    "validation_accuracy = clf.score(X_val, Y_val)\n",
    "print(train_accuracy, validation_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45daecc4-5e96-4e30-856c-4ac25f31d518",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a6d28b7d4d76cbd6747592cbf4d0d76f",
     "grade": false,
     "grade_id": "22-e-task",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "user_expressions": [
     {
      "expression": "PARAM_4",
      "result": {
       "data": {
        "text/plain": "13832"
       },
       "metadata": {},
       "status": "ok"
      }
     }
    ]
   },
   "source": [
    "### Task 2.2-E: Class Balancing and Data Normalization II (5P) \n",
    "\n",
    "While the training accuracy is also satisfying now modify the training data distribution using the parameters of the `generate_distribution` function. Also, remember to train a new neural network model on the new training data (again with a random_state value of {{PARAM_4}}). Discuss how the balanced class distributions and normalized feature value ranges affect the performance of the neural network. In the end use a training data set that allows for a validation accuracy of more than 80\\%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5accb726-366f-4a30-8df0-7abea863eae1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b104078daae68e4563332f1b54f30246",
     "grade": false,
     "grade_id": "22-e-answer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "with threadpool_limits(limits=1, user_api='blas'):\n",
    "    X_tr, Y_tr, X_val, Y_val = generate_distribution((1000, 0.1), 0.1)\n",
    "    # YOUR CODE HERE\n",
    "    clf = train_nnet(X_tr, Y_tr, \n",
    "                     hidden_layer_sizes=(100,),\n",
    "                     activation=\"identity\", \n",
    "                     solver=\"sgd\",\n",
    "                     alpha=0.0001,\n",
    "                     batch_size=200,\n",
    "                     learning_rate=\"constant\",\n",
    "                     max_iter=500,\n",
    "                     learning_rate_init=0.001,\n",
    "                     shuffle=True,\n",
    "                     nesterovs_momentum=True,\n",
    "                     momentum=0.9,\n",
    "                     random_state=13832)\n",
    "    print('Result nnet-Training - Accuracy on training data = ' + '{0:g}%'.format(clf.score(X_tr, Y_tr) * 100) + \"\\n\")\n",
    "    print('Result nnet-Validation - Accuracy on validation data = ' + '{0:g}%'.format(clf.score(X_val, Y_val) * 100) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dca682-6264-4921-8747-94a580d73231",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "52fdf924c007aaa149be98170c5205aa",
     "grade": true,
     "grade_id": "22-e-test",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "validation_accuracy = clf.score(X_val, Y_val)\n",
    "print(validation_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f571d93-443a-4925-b9d0-3d8334b45102",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e0abd17f0b4c26f9e0df5404b435fdea",
     "grade": false,
     "grade_id": "22-f-task",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "user_expressions": []
   },
   "source": [
    "### Task 2.2-F: Parameter Initialization (5P) \n",
    "\n",
    "As you learned, the weights in a neural network are initialized randomly. Accordingly, the same training with the same parameters may yield differing results depending on the initialization. In practice one usually fixes the random seed of all involved libraries to ensure (at least somewhat) reproducible experiments. Try this out yourself by using three arbitrary different values for the `random_state` parameter. Train the models only for 100 iterations and observe the validation accuracy at this point. Train three different models `clf1`,`clf2`, and `clf3` which all yield different validation accuracies. While for this simple problem usually all trainings converge to the global optimum, in more complicated settings, this phenomenon can not only lead to differing convergence speeds but rather to the convergence towards local optima. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea399a7-4fb1-4eed-8ea6-29dccc5d4bd8",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5dd3699addc8ee9d6df535ef2c2cc254",
     "grade": false,
     "grade_id": "22-f-answer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "with threadpool_limits(limits=1, user_api='blas'):\n",
    "    X_tr, Y_tr, X_val, Y_val = generate_distribution((1, 1), 1)\n",
    "    # YOUR CODE HERE\n",
    "    clf1 = train_nnet(X_tr, Y_tr, \n",
    "                      hidden_layer_sizes=(100,),\n",
    "                      activation=\"identity\", \n",
    "                      solver=\"sgd\",\n",
    "                      alpha=0.0001,\n",
    "                      batch_size=200,\n",
    "                      learning_rate=\"constant\",\n",
    "                      max_iter=100,\n",
    "                      learning_rate_init=0.001,\n",
    "                      shuffle=True,\n",
    "                      nesterovs_momentum=True,\n",
    "                      momentum=0.9,\n",
    "                      random_state=42)\n",
    "    clf2 = train_nnet(X_tr, Y_tr, \n",
    "                      hidden_layer_sizes=(100,),\n",
    "                      activation=\"identity\", \n",
    "                      solver=\"sgd\",\n",
    "                      alpha=0.0001,\n",
    "                      batch_size=200,\n",
    "                      learning_rate=\"constant\",\n",
    "                      max_iter=100,\n",
    "                      learning_rate_init=0.001,\n",
    "                      shuffle=True,\n",
    "                      nesterovs_momentum=True,\n",
    "                      momentum=0.9,\n",
    "                      random_state=123)\n",
    "    clf3 = train_nnet(X_tr, Y_tr, \n",
    "                      hidden_layer_sizes=(100,),\n",
    "                      activation=\"identity\", \n",
    "                      solver=\"sgd\",\n",
    "                      alpha=0.0001,\n",
    "                      batch_size=200,\n",
    "                      learning_rate=\"constant\",\n",
    "                      max_iter=100,\n",
    "                      learning_rate_init=0.001,\n",
    "                      shuffle=True,\n",
    "                      nesterovs_momentum=True,\n",
    "                      momentum=0.9,\n",
    "                      random_state=456)\n",
    "\n",
    "    print('Result nnet-Training - Accuracy on training data = ' + '{0:g}%'.format(clf.score(X_tr, Y_tr) * 100) + \"\\n\")\n",
    "    print('Result nnet-Validation - Accuracy on validation data = ' + '{0:g}%'.format(clf.score(X_val, Y_val) * 100) + \"\\n\")\n",
    "    \n",
    "    print('Result nnet-Training - Accuracy on training data = ' + '{0:g}%'.format(clf2.score(X_tr, Y_tr) * 100) + \"\\n\")\n",
    "    print('Result nnet-Validation - Accuracy on validation data = ' + '{0:g}%'.format(clf2.score(X_val, Y_val) * 100) + \"\\n\")\n",
    "    \n",
    "    print('Result nnet-Training - Accuracy on training data = ' + '{0:g}%'.format(clf3.score(X_tr, Y_tr) * 100) + \"\\n\")\n",
    "    print('Result nnet-Validation - Accuracy on validation data = ' + '{0:g}%'.format(clf3.score(X_val, Y_val) * 100) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220156c1-170c-4522-8633-b6f8db049101",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b24f113e0b181ff7470a6180816c321f",
     "grade": true,
     "grade_id": "22-f-test",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "validation_accuracy1 = clf1.score(X_val, Y_val)\n",
    "validation_accuracy2 = clf2.score(X_val, Y_val)\n",
    "validation_accuracy3 = clf3.score(X_val, Y_val)\n",
    "print(validation_accuracy1, validation_accuracy2, validation_accuracy3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4019286-e3ca-4ceb-b54a-1e89ed686803",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "scenes_data": {
   "active_scene": "Default Scene",
   "init_scene": "Default Scene",
   "scenes": [
    "Default Scene"
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
